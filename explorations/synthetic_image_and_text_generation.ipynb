{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTkAE4G8w/m/84mdn2tLV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajarhomayouni/synthetic_multimodal_data_generation/blob/main/synthetic_image_and_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image to text transformer"
      ],
      "metadata": {
        "id": "1yqwU1dg63Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First image to text transformer model"
      ],
      "metadata": {
        "id": "pL2RTHBX7K8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_resuYBGaIqf"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/docs/transformers/model_doc/visual_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "UQc_MCTRaVV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image to text transformer\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import GPT2TokenizerFast, ViTFeatureExtractor, VisionEncoderDecoderModel\n",
        "\n",
        "# load a fine-tuned image captioning model and corresponding tokenizer and feature extractor\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sOgOFcjVaOO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's perform inference on an image\n",
        "#url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "url=\"/content/DATA/DATA/1/person1133_virus_1865.jpeg\"\n",
        "#image = Image.open(requests.get(url, stream=True).raw)\n",
        "image = Image.open(url)\n",
        "pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "# autoregressively generate caption (uses greedy decoding by default)\n",
        "generated_ids = model.generate(pixel_values)\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "6pcZ2UKd-8fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second image to text transformer model"
      ],
      "metadata": {
        "id": "yEaYZWo_8rhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image to text transformer\n",
        "#https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "\n",
        "# load image from the IAM dataset\n",
        "url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "# training\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
        "text = \"hello world\"\n",
        "labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "loss = outputs.loss\n",
        "\n",
        "# inference (generation)\n",
        "generated_ids = model.generate(pixel_values)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "generated_text"
      ],
      "metadata": {
        "id": "5JsKmyIa_o2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to Text transformer\n",
        "\n",
        "Note: We may need it later"
      ],
      "metadata": {
        "id": "LkGZLbLz6-Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sentences=generated_text"
      ],
      "metadata": {
        "id": "e3pt_8t_-yps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text to text transformer (generate fake text)\n",
        "\n",
        "from transformers import EncoderDecoderModel, BertTokenizer,BertConfig, EncoderDecoderConfig, AutoTokenizer\n",
        "\n",
        "#Model Initialization**************************************************************\n",
        "# First way: Bert default pretrained model\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert\n",
        "\n",
        "\n",
        "# Second way: pretrained sample model\n",
        "tokenizer = AutoTokenizer.from_pretrained('patrickvonplaten/bert2bert_cnn_daily_mail')\n",
        "model = EncoderDecoderModel.from_pretrained('patrickvonplaten/bert2bert_cnn_daily_mail')\n",
        "\n",
        "# Third way: Default Bert config\n",
        "\"\"\"config_encoder = BertConfig()\n",
        "config_decoder = BertConfig()\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
        "model = EncoderDecoderModel(config=config)\"\"\"\n",
        "\n",
        "# Another: GPT2\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "#Model Training**************************************************************\n",
        "input_ids = tokenizer(batch_sentences, padding='max_length', max_length=15, truncation=True, return_tensors=\"pt\").input_ids\n",
        "outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
        "loss, outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)[:2]\n",
        "\n",
        "#Model Evaluation**************************************************************\n",
        "generated_ids = model.generate(input_ids)\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "generated_text"
      ],
      "metadata": {
        "id": "CxleU9l7-KlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image to 1D feature vector transformer\n",
        "\n",
        "Note: May need later"
      ],
      "metadata": {
        "id": "ne6VmN5t7tlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from image\n",
        "# https://huggingface.co/docs/transformers/model_doc/vit\n",
        "# https://huggingface.co/google/vit-base-patch16-224\n",
        "\n",
        "from transformers import ViTFeatureExtractor, ViTModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"huggingface/cats-image\")\n",
        "image = dataset[\"test\"][\"image\"][0]\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "outputs"
      ],
      "metadata": {
        "id": "bv2dP0ycA0qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image to Image Transformer"
      ],
      "metadata": {
        "id": "8zPuvjMr7e5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First image to image transformer model\n",
        "\n",
        "Limitation: Low quality of output image"
      ],
      "metadata": {
        "id": "4JAmpGfx7itx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image to image transformer (image generation)\n",
        "# https://huggingface.co/docs/transformers/model_doc/imagegpt\n",
        "# https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT\n",
        "\n",
        "\n",
        "# This is unconditional, I need the conditional approach\n",
        "\n",
        "from transformers import ImageGPTFeatureExtractor, ImageGPTForCausalImageModeling\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "feature_extractor = ImageGPTFeatureExtractor.from_pretrained(\"openai/imagegpt-small\")\n",
        "model = ImageGPTForCausalImageModeling.from_pretrained(\"openai/imagegpt-small\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# unconditional generation of 8 images\n",
        "batch_size = 8\n",
        "context = torch.full((batch_size, 1), model.config.vocab_size - 1)  # initialize with SOS token\n",
        "context = torch.tensor(context).to(device)\n",
        "output = model.generate(\n",
        "    input_ids=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40\n",
        ")\n",
        "\n",
        "clusters = feature_extractor.clusters\n",
        "n_px = feature_extractor.size\n",
        "\n",
        "samples = output[:, 1:].cpu().detach().numpy()\n",
        "samples_img = [\n",
        "    np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [n_px, n_px, 3]).astype(np.uint8) for s in samples\n",
        "]  # convert color cluster tokens back to pixels\n",
        "f, axes = plt.subplots(1, batch_size, dpi=300)\n",
        "\n",
        "for img, ax in zip(samples_img, axes):\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(img)"
      ],
      "metadata": {
        "id": "nBgLO7DrDYvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second image to image transformer model. (Image generation based on Style GAN)"
      ],
      "metadata": {
        "id": "36NOA8_T73J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stylegan2_pytorch"
      ],
      "metadata": {
        "id": "gIVDSzmyrLvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/podgorskiy/dnnlib"
      ],
      "metadata": {
        "id": "YOtAkhRJsf4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image to Image  \n",
        "#Image generation : https://heartbeat.comet.ml/stylegans-use-machine-learning-to-generate-and-customize-realistic-images-c943388dc672\n",
        "# Had error you can work on it\n",
        "# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
        "# 4.0 International License. To view a copy of this license, visit\n",
        "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
        "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
        "\n",
        "\"\"\"Minimal script for generating an image using pre-trained StyleGAN generator.\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import config\n",
        "\n",
        "def main():\n",
        "    # Initialize TensorFlow.\n",
        "    tflib.init_tf()\n",
        "\n",
        "    # Load pre-trained network.\n",
        "    url = 'https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ' # karras2019stylegan-ffhq-1024x1024.pkl\n",
        "    with dnnlib.util.open_url(url, cache_dir=config.cache_dir) as f:\n",
        "        _G, _D, Gs = pickle.load(f)\n",
        "        # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
        "        # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
        "        # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n",
        "\n",
        "    # Print network details.\n",
        "    Gs.print_layers()\n",
        "\n",
        "    # Pick latent vector.\n",
        "    rnd = np.random.RandomState(5)\n",
        "    latents = rnd.randn(1, Gs.input_shape[1])\n",
        "\n",
        "    # Generate image.\n",
        "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    images = Gs.run(latents, None, truncation_psi=0.7, randomize_noise=True, output_transform=fmt)\n",
        "\n",
        "    # Save image.\n",
        "    os.makedirs(config.result_dir, exist_ok=True)\n",
        "    png_filename = os.path.join(config.result_dir, 'example.png')\n",
        "    PIL.Image.fromarray(images[0], 'RGB').save(png_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "0vPxwqyJBsXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third image to image transformer model. (Image generation based on Style GAN)\n",
        "\n",
        "Note: This model was successfully used by my student to generate COVID-19 X-Ray Images. Her code is available: https://colab.research.google.com/drive/1Z_VHdxNcPlsMPCzmBFTvW0LUZQawbQ9u?usp=sharing"
      ],
      "metadata": {
        "id": "LEBkCp9M8K3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"hajarhomayouni\",\"key\":\"33b81fa143f6c6b4c0eea1e591a9f383\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "HE3SJMOTp_01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada.git"
      ],
      "metadata": {
        "id": "OhdXp-XDtJii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stylegan2-ada/"
      ],
      "metadata": {
        "id": "9fjkxPS0tXTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33331c7-8488-4c49-82bc-0a38e8f60797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os, sys\n",
        "\n",
        "path = \"/content/data/\"\n",
        "dirs = os.listdir(path)\n",
        "\n",
        "\n",
        "for item in dirs:\n",
        "    if os.path.isfile(path+item):\n",
        "        im = Image.open(path+item)\n",
        "        f, e = os.path.splitext(path+item)\n",
        "        imResize = im.resize((512,512), Image.ANTIALIAS)\n",
        "        imResize.save(\"/content/pre/\" + item + '.jpeg', 'JPEG', quality=90)"
      ],
      "metadata": {
        "id": "-JIsZgPMuq6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset_tool.py create_from_images /content/output /content/pre/"
      ],
      "metadata": {
        "id": "wNZwL25Jtcaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRkJeyw9Qn05"
      },
      "outputs": [],
      "source": [
        "!pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "metadata": {
        "id": "-Soq--JHtjNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==1.14.0"
      ],
      "metadata": {
        "id": "ztoTRONq0UYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy"
      ],
      "metadata": {
        "id": "L5mJVlSQ1ihE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.19.5"
      ],
      "metadata": {
        "id": "s6qgd3Qaui5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --outdir /content/generated --snap=10 --data=/content/output --augpipe=bgcfnc --res=512"
      ],
      "metadata": {
        "id": "hfw2_cv5v_XD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}